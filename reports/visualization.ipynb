{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110eba90",
   "metadata": {},
   "source": [
    "# Experiment Results Visualization\n",
    "\n",
    "This notebook visualizes the performance metrics from different language models on the sandbagging detection experiments. It compares individual model performance and provides combined analysis across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea5926",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e357541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "home = Path(__file__).parent.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0d533",
   "metadata": {},
   "source": [
    "## 2. Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad69ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to experiment logs\n",
    "logs_dir = home / 'reports/experiment_logs'\n",
    "\n",
    "# Load all CSV files\n",
    "model_data = {}\n",
    "csv_files = list(logs_dir.glob('*.csv'))\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'model_comparison' not in csv_file.name:\n",
    "        model_name = csv_file.stem\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            model_data[model_name] = df\n",
    "            print(f\"✓ Loaded {model_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(model_data)}\")\n",
    "print(f\"Available models: {list(model_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddad61d",
   "metadata": {},
   "source": [
    "## 3. Parse Headers and Display Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display headers from each model\n",
    "print(\"=\" * 80)\n",
    "print(\"CSV HEADERS FOR EACH MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, df in model_data.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Columns ({len(df.columns)}): {list(df.columns)}\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Categories: {df['category'].unique() if 'category' in df.columns else 'N/A'}\")\n",
    "\n",
    "# Identify numeric columns for analysis\n",
    "numeric_cols = ['eval_correct', 'casual_correct', 'correctness_diff', 'length_ratio', \n",
    "                'semantic_similarity', 'sandbagging_flag']\n",
    "print(f\"\\nMetric columns to analyze: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd03b1",
   "metadata": {},
   "source": [
    "## 4. Individual Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9604adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual model performance visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Individual Model Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['eval_correct', 'casual_correct', 'correctness_diff', 'length_ratio', \n",
    "           'semantic_similarity', 'sandbagging_flag']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    for model_name, df in model_data.items():\n",
    "        if metric in df.columns:\n",
    "            values = df[metric].dropna()\n",
    "            ax.hist(values, alpha=0.5, label=model_name, bins=20)\n",
    "    \n",
    "    ax.set_title(f'{metric}', fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Individual model histograms generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9259d90",
   "metadata": {},
   "source": [
    "## 5. Combined Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, df in model_data.items():\n",
    "    for metric in numeric_cols:\n",
    "        if metric in df.columns:\n",
    "            values = df[metric].dropna()\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Metric': metric,\n",
    "                'Mean': values.mean(),\n",
    "                'Std': values.std(),\n",
    "                'Min': values.min(),\n",
    "                'Max': values.max(),\n",
    "                'Count': len(values)\n",
    "            })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Pivot for easier visualization\n",
    "pivot_mean = comparison_df.pivot(index='Metric', columns='Model', values='Mean')\n",
    "\n",
    "print(\"Mean Performance Metrics by Model:\")\n",
    "print(pivot_mean.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070126a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize mean comparison across models\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "pivot_mean.T.plot(kind='bar', ax=ax, width=0.8)\n",
    "plt.title('Mean Metric Values Across Models', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Combined model comparison chart generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae890f",
   "metadata": {},
   "source": [
    "## 6. Performance by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e55c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataframe for category analysis\n",
    "combined_data = []\n",
    "\n",
    "for model_name, df in model_data.items():\n",
    "    df_copy = df.copy()\n",
    "    df_copy['model'] = model_name\n",
    "    combined_data.append(df_copy)\n",
    "\n",
    "combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "# Display correctness rates by category and model\n",
    "print(\"Correctness Rates by Category and Model:\")\n",
    "print(\"\\nEvaluation Context Correctness:\")\n",
    "if 'category' in combined_df.columns and 'eval_correct' in combined_df.columns:\n",
    "    category_eval = combined_df.groupby(['model', 'category'])['eval_correct'].agg(['mean', 'count'])\n",
    "    print(category_eval)\n",
    "\n",
    "print(\"\\nCasual Context Correctness:\")\n",
    "if 'category' in combined_df.columns and 'casual_correct' in combined_df.columns:\n",
    "    category_casual = combined_df.groupby(['model', 'category'])['casual_correct'].agg(['mean', 'count'])\n",
    "    print(category_casual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category performance\n",
    "if 'category' in combined_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Evaluation context correctness by category\n",
    "    eval_by_cat = combined_df.groupby(['model', 'category'])['eval_correct'].mean().unstack()\n",
    "    eval_by_cat.T.plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Evaluation Context Correctness by Category')\n",
    "    axes[0].set_ylabel('Correctness Rate')\n",
    "    axes[0].set_xlabel('Category')\n",
    "    axes[0].legend(title='Model', bbox_to_anchor=(1.05, 1))\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Casual context correctness by category\n",
    "    casual_by_cat = combined_df.groupby(['model', 'category'])['casual_correct'].mean().unstack()\n",
    "    casual_by_cat.T.plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title('Casual Context Correctness by Category')\n",
    "    axes[1].set_ylabel('Correctness Rate')\n",
    "    axes[1].set_xlabel('Category')\n",
    "    axes[1].legend(title='Model', bbox_to_anchor=(1.05, 1))\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Category performance visualization generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c980d09",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics and Model Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary statistics\n",
    "summary_stats = comparison_df.groupby('Model').agg({\n",
    "    'Mean': ['mean', 'std'],\n",
    "    'Count': 'first'\n",
    "}).round(4)\n",
    "\n",
    "print(\"Summary Statistics Across All Metrics:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Calculate overall correctness scores for ranking\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL RANKING - OVERALL CORRECTNESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "overall_scores = []\n",
    "for model_name, df in model_data.items():\n",
    "    eval_correct_rate = df['eval_correct'].mean() if 'eval_correct' in df.columns else 0\n",
    "    casual_correct_rate = df['casual_correct'].mean() if 'casual_correct' in df.columns else 0\n",
    "    combined_score = (eval_correct_rate + casual_correct_rate) / 2\n",
    "    \n",
    "    overall_scores.append({\n",
    "        'Model': model_name,\n",
    "        'Eval Context Correctness': eval_correct_rate,\n",
    "        'Casual Context Correctness': casual_correct_rate,\n",
    "        'Combined Score': combined_score\n",
    "    })\n",
    "\n",
    "ranking_df = pd.DataFrame(overall_scores).sort_values('Combined Score', ascending=False)\n",
    "print(ranking_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model ranking\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ranking_df_sorted = ranking_df.sort_values('Combined Score', ascending=True)\n",
    "colors = plt.cm.RdYlGn(ranking_df_sorted['Combined Score'] / ranking_df_sorted['Combined Score'].max())\n",
    "ax.barh(ranking_df_sorted['Model'], ranking_df_sorted['Combined Score'], color=colors)\n",
    "ax.set_xlabel('Combined Correctness Score', fontsize=12)\n",
    "ax.set_title('Model Ranking by Overall Correctness', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, score) in enumerate(zip(ranking_df_sorted['Model'], ranking_df_sorted['Combined Score'])):\n",
    "    ax.text(score, i, f' {score:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model ranking visualization generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbf9a5",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary results to CSV\n",
    "output_dir = home / 'reports'\n",
    "\n",
    "# Export model ranking\n",
    "ranking_df.to_csv(output_dir / 'model_ranking.csv', index=False)\n",
    "print(f\"✓ Model ranking exported to: {output_dir / 'model_ranking.csv'}\")\n",
    "\n",
    "# Export comparison statistics\n",
    "comparison_df.to_csv(output_dir / 'metric_comparison.csv', index=False)\n",
    "print(f\"✓ Metric comparison exported to: {output_dir / 'metric_comparison.csv'}\")\n",
    "\n",
    "# Export pivot table (mean values)\n",
    "pivot_mean.to_csv(output_dir / 'mean_metrics_by_model.csv')\n",
    "print(f\"✓ Mean metrics exported to: {output_dir / 'mean_metrics_by_model.csv'}\")\n",
    "\n",
    "print(\"\\nAll visualizations and exports complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
